\section{Intro}
\subsection{Spazi Normati}
Sia \(X\) uno spazio vettoriale su campo \(\mathbb{K}\) (\(\mathbb{C}\) o \(\mathbb{R}\)). 
\begin{definition}{norma}
    Si definisce \textbf{norma} una funzione
    \[
      \|\cdot \|: X \to \mathbb{R}_{\ge 0} 
    \]
    tale che
\begin{enumerate}[label = \roman*.]
    \item \(\|x\|=0 \iff x=0\) 
    \item \(\|\lambda x\| = |\lambda|\|x\|\), \(\forall \lambda \in \mathbb{K}\) e \(\forall x \in X\) 
    \item \(\|x + y\| \le \|x\| + \|y\|\), \(\forall x, y \in X\) 
\end{enumerate}
\end{definition}

\begin{definition}{Spazio Normato}
    Uno \textbf{spazio normato} è una coppia \({(X, \|\cdot \|)}\) tale che \(X\) sia uno spazio vettoriale e \(\|\cdot \|\) una norma su \(X\).
\end{definition}
Per una notazione più leggera, quando non è ambiguo sottintenderemo la norma,
scrivendo ``sia \(X\) uno spazio normato''.

\begin{proposition}[Metrica indotta da \(\|\cdot \|\)]
    La norma \(\|\cdot \|\) induce su \(X\) una metrica
    \[
      d{(x, y)} = \|x - y\|\quad\quad \forall x, y \in X
    \]
\end{proposition}

\begin{note}[zioni]
    Alcune notazioni utili: 
\begin{itemize}[label = --]
    \item \(B_r{(x_{0})} = \{x \in X : \|x - x_{0}\| \le r\} = x_{0} + r\,B_1{(0)} \)
    \item \(\partial B_r{(x_{0})} = \{x \in X : \|x - x_{0}\| = r\} \) 
\end{itemize}
\end{note}

\begin{definition}{Convergenza in norma - Convergenza forte}
    Sia \(\{x_{n}\}_{n \in \mathbb{N}} \) una successione in \(X\) e sia \(x \in X\). Dico che \(x_{n}\) converge a \(x\) in norma o fortemente se 
    \[
      \forall \varepsilon > 0 \,\,\exists \overline{n}\, :\, \|x_{n} - x\| \le \varepsilon \quad \forall n \ge \overline{n}
    \]
\end{definition}

\begin{definition}{Successione di Cauchy}
    Una successione \(\{x_{n}\}_{n \in \mathbb{N}} \subseteq X \) è detta di
    Cauchy se
    \[
      \forall \varepsilon >0 \,\, \exists \overline{n} \,:\, \|x_{n} - x_{m}\| \le \varepsilon \quad \forall n,m \ge \overline{n}
    \]
\end{definition}

\begin{remark}{}
    La norma \(\|\cdot \|\) è una funzione continua.
\end{remark}
\begin{proof}{}
    Preso \(x,y \in X\),
    \[
      \|x\| = \|x - y + y\| \le \|x -y\| + \|y\|
    \]
    e similmente si può con variabili scambiate. Ne consegue che
    \[
      \left| \|x\| - \|y\| \right| \le \|x - y\|
    \]
    dunque la norma è Lipschitziana con costante 1
\end{proof}

\begin{definition}{Norma equivalente}
    Sia \(X\) uno spazio normato e siano \(\|\cdot \|_{1} \) e \(\|\cdot \|_2\)
    due norme su \(X\). Dico che \(\|\cdot \|_1\) è \textbf{topologicamente
    equivalente} a \(\|\cdot \|_2\) se
    \begin{align*}
        \forall x \in X \,\, \forall r > 0 \,\, \exists r_{1}, r_{2} > &0 : \\
        B_{r_{1}}{(x, \|\cdot \|_1)} \subseteq B_{r}{(x, \|\cdot \|_2)} &\text{ e } B_{r_{2}} {(x, \|\cdot \|_2)} \subseteq B_{r} {(x, \|\cdot \|_1)} 
    \end{align*}
\end{definition}

\begin{proposition}{}
    Sia \(X\) normato. Allora due norme \(\|\cdot \|_1\) e \(\|\cdot \|_2\) sono
    equivalenti se e solo se \(\exists \alpha, \beta > 0\) tali che
    \[
      \alpha \|x\|_1 \le \|x\|_2 \le \beta \|x\|_1 \quad \forall x \in X
    \]
\end{proposition}
\begin{proof}\( \)
\begin{itemize}
    \item[\(\implies \)] Fissato \(x_{0} = 0\), preso \(r\) tale che 
        \[
          B_{r_{2}} {(0, \|\cdot \|_2)} \subseteq B_r{(0, \|\cdot \|_1)} 
        \]
        preso ora \(0\neq x \in X\), sia \(y := \frac{r_{2}}{2 \|x\|_2}x\), così
        che \(\|y\|_2 = \frac{r_{2}}{2} \), dunque \(y \in 
        B_{r_{2}} {(0, \|\cdot \|_2)}\) e quindi per l'inclusione sopra
        \[
          \|y\|_1 = \frac{r_{2}}{2} \frac{\|x\|_1}{\|x\|_2} \le r 
        \]
        che è la prima delle disuguaglianze richieste. Similmente si può trovare
        l'altra scambiando \(x\) e \(y\), le due norme, e \(r_{2}\) con \(r_{1}\) 
    \item[\(\impliedby \)] Preso \(x_{0} \in X\) e \(r > 0\), sia \(r_{1} := r /
        \beta\). Allora, per ogni \(x \in X\) 
        \[
          \|x - x_{0}\|_1 \le \frac{r}{\beta} \implies \|x-x_{0}\|_2 \le \beta
          \|x - x_{0}\|_1 \le r
        \]
        che è la prima delle inclusioni richieste. Similmente si può trovare
        l'altra prendendo \(r_{2} := r / \alpha\) e scambiando le norme.
\end{itemize}
\end{proof}

\begin{remark}{}
    Se \(\{x_{n}\} \) è di Cauchy rispetto alla norma \(\|\cdot \|_1\) e \(\|\cdot \|_2\) è una norma equivalente alla prima, allora \(\{x_{n}\} \) è di Cauchy rispetto a \(\|\cdot \|_2\) 
\end{remark}

\begin{definition}{Dimensione}
    Sia \(X\) uno spazio vettoriale. Allora 
    \[
      \dim X = \begin{cases}{}
          0 & X = \{0\} \\
          n & n \in \mathbb{N} \text{ e \(X\) ha una base di \(n\) elementi} \\
          +\infty & \forall n \in \mathbb{N}, \text{ esistono \(n\) vettori linearmente indipendenti}
      \end{cases}
    \]
\end{definition}


\begin{theorem}[Equivalenza delle norme]
    Sia \(X\) uno spazio vettoriale di dimensione finita. Allora tutte le norme
    su \(X\) sono topologicamente equivalenti.
\end{theorem}
\begin{proof}{}
    Sia \(\{e_{1}, \dots, e_{n}\} \) una base di \(X\). Sia \(x \in X\). Allora
    sia
    \begin{equation*}
      x = \sum_{i=1}^{n} x^{i}e_{i} \quad \text{ con } x^{i} \in \mathbb{K} \quad \forall i
      \in \{1, \dots, n\}  
    \end{equation*}
    Definiamo la norma (facile controllo lasciato come esercizio)
    \[
      \|x\|_1 = \sum_{i=1}^{n} | a^{i}|  
    \]
    Sia ora \(\|\cdot \|\) un'altra norma su \(X\), dimostriamo che \(\|\cdot \|\) è equivalente a \(\|\cdot \|_1\).
    \begin{equation*}
        \|x\| = \left\| \sum_{i=1}^{n} x^{i}e_{i} \right\| \le \sum_{i=1}^{n} |x^{i}| \|e_{i}\| \le \underbrace{{\left( \max_{1 \le i \le N} \|e_{i}\|  \right)}}_{\beta} \|x\|_1
    \end{equation*}
    Rimane da dimostrare che \(\exists \alpha > 0 \) tale che \(\|x\|_1 \le \frac{1}{\alpha}\|x\|\). Assumiamo per assurdo che \(\forall n \in \mathbb{N}\) esista \(x_{n} \in X\) tale che \(\|x_{n}\|_1 > n \|x_{n}\|\). Prendiamo ora (ovviamente \(x_{n} \neq 0\) per la diseguaglianza stretta)
    \[
      y_{n} := \frac{x_{n}}{\|x_{n}\|_1} \text{ per ogni \(n \in \mathbb{N}\) }
      \implies \|y_{n}\| < \frac{1}{n} \quad ; \quad \|y_n\|_1 = 1
    \]
    Dalla seconda otteniamo che \(\forall i \in \{1, \dots, n\} \) e \(\forall n \in \mathbb{N}\),  \(| y_{n}^{i} | \le 1\). Per Bolzano-Weierstrass esiste una sottosuccessione \(n_k\) tale che per ogni \(i \in \{1, \dots, n\} \), \(y_{n_k}^{i} \to y^{i} \).

    Allora
    \[
      \|y_{n_k} - y \| \le \beta\|y_{n_k} -y\|_1 = \beta\left\| \sum_{i=1}^{n} {\left( y_{n_k}^{i} - y^{i} \right)}  e_i \right\|_1 \le \beta^2 \sum_{i=1}^{n} |y_{n_k}^{i} - y^{i}| \overset{k \to \infty}{\longrightarrow} 0
    \]
    e poiché
    \[
      1 = \|y_{n_k} \| \le \|y_{n_k} - y\| + \|y\| \overset{k \to \infty}{\implies } \|y\| \ge  1
    \]
    che è in contraddizione con \(\|y_{n}\| \to 0\)
\end{proof}

\begin{definition}{Spazio di Banach}
    \(X\) spazio normato è detto \textbf{spazio di Banach} se le successioni di
    Cauchy convergono in \(X\) (ossia \(X\) è completo)
\end{definition}
\begin{theorem}{}
    Sia \(X\) uno spazio normato di dimensione finita. Allora \(X\) è di Banach.
\end{theorem}
\begin{proof}{}
    Sia \(N = \dim X\). Dimostro che \(X\) è completo secondo la norma \(\|\cdot \|_1\).
    
    Sia \(\{x_{n}\} \) una successione di Cauchy. Vogliamo mostrare l'esistenza
    di \(x \in X\) tale che \(\lim_{n \to \infty} \|x_{n}-x\|_1 = 0\).
    Da definizione di successione di Cauchy, 
    \[
      \forall \varepsilon > 0 \,\, \exists \overline{n} \in \mathbb{N}: \forall
      n,m \ge \overline{n},\,\, \|x_{n}-x_{m}\|_1 = \sum_{i=1}^{N} \left| x_{n}^{i}-x_{m}^{i}\right| \le \varepsilon
    \]
    per cui ogni successione delle componenti \(\{x^{i}_{n}\}_{n \in \mathbb{N}}  \) è di Cauchy in \(\mathbb{K}\). Poiché \(\mathbb{C}\) e \(\mathbb{R}\) sono completi, allora \(x_{n}^{i} \to x^{i} \in \mathbb{K}\) per ogni \(i \in \{1,\dots,N\} \). Concludiamo osservando che
    \[
      \|x_{n}-x\|_1 = \sum_{i=1}^{N} \left| x_{n}^{i} - x^{i}\right| \overset{n\to \infty}{\longrightarrow} 0
    \]
\end{proof}

\begin{example}{}
    Sia \(X = \mathbb{K}^{N}\) con \(\mathbb{K} = \mathbb{C}\) o
    \(\mathbb{R}\). Su tale spazio possiamo avere le norme
    \[
      \|x\|_p = {\left( \sum_{i=1}^{N} |x^{i}|^{p}  \right)}^{\frac{1}{p}},
      \quad p \in [1, \infty)
    \]
    \(X\) è chiaramente di Banach.
\end{example}
\begin{example}{}
    Sia \(\Omega \subseteq \mathbb{R}^{d} \), allora \(X = C^{0}{(\Omega,\mathbb{K}^{N})}\) spazio delle funzioni continue \(\Omega\to \mathbb{R}^{N}\). Facile verificare che \(X\) formi uno spazio vettoriale.

    Preso ora \(\Omega\) aperto e limitato. 
    \[
      C^{0}{(\overline{\Omega})} = \{f:\Omega\to \mathbb{R} \text{ unif.
      continue}\} 
    \]
    poiché \(f\) è uniformemente continua se e solo se si può estendere con coninuità al bordo.
    Si può prendere la norma
    \[
      \|f\|_{\infty} = \max_{x \in \overline{\Omega}} |f{(x)}| \quad \forall f
      \in C^{0}{(\overline{\Omega})}
    \]
    che si può verificare essere effettivamente una norma. Inoltre con tale
    norma \(C^{0}{(\overline{\Omega})}\) è uno spazio di Banach.

    Le funzioni in \(C^{0}{(\overline{\Omega})}\) sono limitate e definite su un
    compatto, dunque sono anche integrabili, e possiamo dunque definire le norme
    \[
        \|f\|_p = {\left( \int_{\Omega} |f{(x)}|^{p} \,d \right)}^{\frac{1}{p}}
        \quad \forall f \in C^{0}{(\overline{\Omega})} \quad \forall p \in [1,
        \infty)
    \]
    ma per nessun \(p\) la norma rende \(C^{0}{(\overline{\Omega})}\) completo.
    Un esempio è
    \[
      f_{n}{(x)} = \begin{cases}{}
          1 & x \in [0,\frac{1}{2} - \frac{1}{n}] \\
          \text{lineare} & x \in [\frac{1}{2} - \frac{1}{n}, \frac{1}{2}+\frac{1}{n}] \\
          0 & x \in [\frac{1}{2}+\frac{1}{n},1]
      \end{cases}
    \]
    definita in \([0,1]\). Tale funzione converge in \(L_p\) con la stessa norma a una funzione non
    continua.
\end{example}

\begin{example}{}
    Legato all'esempio precedente, con la stessa norma gli spazi \\\(L^{p}{(\Omega,\mu)}\) sono spazi di Banach.

    Presi ora gli spazi \(l^{p} := L^{p}{(\mathbb{N},\#)}\) gli spazi di
    successioni \(\mathbb{N}\to \mathbb{K}\), abbiamo che anch'essi sono spazi
     di Banach con norma
     \begin{align*}
         \|x\|_p &:= \|x\|_{L^{p}{(\mathbb{N}, \#)}} = {\left( \int_{\mathbb{N}} |x{(n)}|^{p} \,d\# \right)}^{\frac{1}{p}} = {\left( \sum_{n=1}^{\infty} |x{(n)}|^{p}  \right)}^{\frac{1}{p}} \\
         \|x\|_{\infty} &= \sup_{n \in \mathbb{N}} |x{(n)}|
     \end{align*}
\end{example}
\begin{note}[\emph{zione}]
    Per le successioni in \(l^{p}\), indicheremo \(x \in l^{p}\)
    intendendola come funzione \(x: \mathbb{N}\to \mathbb{K}\), per cui per
    indicare la componente \(n\)-esima di \(x\) indicheremo \(x{(n)}\). In tal
    modo possiamo indicare le successioni di elementi in \(l^{p}\) come
    successioni \(\{x_{n}\}_{n \in \mathbb{N}}\), dove ogni \(x_{n}: \mathbb{N}\to \mathbb{K}\) è una funzione in \(l^{p}\) 
\end{note}

\subsection{Spazi di Hilbert}
\begin{definition}{Prodotto scalare}

    Sia \(X\) uno spazio vettoriale su \(\mathbb{C}\). Allora un prodotto
    scalare è un'applicazione \(\Span{\cdot ,\cdot } : X\times X \to \mathbb{C}\) tale che
\begin{enumerate}[label = \roman*.]
    \item \(\Span{x,y} = \overline{\Span{y,x} } \quad \forall x,y \in X\) 
    \item \(\Span{x,x} \ge 0\) e \(\Span{x,x} = 0 \iff x = 0\) 
    \item \(\Span{\alpha x + \beta y, z} = \alpha \Span{x,z} + \beta \Span{y,z}, \quad \forall \alpha,\beta \in \mathbb{C}, \quad \forall x,y,z \in X \)
\end{enumerate}
\end{definition}
\begin{remark}{}
    Gli stessi assiomi valgono anche sul prodotto scalare su spazio reale.
    Semplicemente si ha che se \(x \in \mathbb{R}\), allora \(\overline{x} = x\) quindi si possono droppare tutti i coniugati e viene tutto più leggero.
\end{remark}
\begin{note}[antilinearità nella seconda componente]
    \[
      \Span{x, \alpha y + \beta z} \overset{i.}{=} \overline{\Span{\alpha y + \beta z, x} } \overset{iii.}{=} \overline{\alpha} \overline{\Span{y,x} } + \overline{\beta}\overline{\Span{z,x} } \overset{i.}{=}
      \overline{\alpha} \Span{x,y} + \overline{\beta}\Span{x,z} 
    \]
\end{note}

\begin{lemmao}[Diseguaglianza di Cauchy-Schwarz]
    Sia \(X\) uno spazio vettoriale munito del prodotto scalare \(\Span{\cdot ,\cdot } \). Allora
    \[
      |\Span{x,y} |^2 \le \Span{x,x} \Span{y,y} \quad \forall x,y \in X
    \]
    e inoltre la diseguaglianza è un'uguaglianza se e solo se \(x\) e \(y\) sono
    linearmente dipendenti.
\end{lemmao}
\begin{proof}{}
    Sia \(z := \Span{y,y} x - \Span{x,y} y\). Allora 
    \begin{align*}
        0 \le \Span{z,z} &= \Span{\Span{y,y} x - \Span{x,y} y, \Span{y,y} x - \Span{x,y} y} = \\
                &= \Span{y,y} \Span{x, \Span{y,y} x - \Span{x,y} y} - \Span{x,y} \Span{y, \Span{y,y} x - \Span{x,y} y} = \\
                &= \Span{y,y}^2\Span{x,x} - \Span{y,y} \Span{y,x} \Span{x,y} -
                \cancel{\Span{x,y} \Span{y,y} \Span{y,x}} + \cancel{\Span{x,y} \Span{y,x} \Span{y,y}} = \\
                &= \Span{y,y} {\left(\Span{y,y} \Span{x,x} -
                    |\Span{x,y}|^2\right)}
    \end{align*}
    quindi ora o \(y=0\) che farebbe valere la tesi, oppure si può semplificare
    \(\Span{y,y} \) e rimane esattamente la tesi.

    Infine si verifica l'uguaglianza quando \(z=0\), ossia quando \(x\) e \(y\)
    sono collineari.
\end{proof}

\begin{definition}{Spazio prehilbertiano}
    Uno spazio vettoriale \(X\) con prodotto scalare viene detto spazio \textbf{prehilbertiano} (o spazio \emph{con prodotto interno})
\end{definition}
\begin{example}{}
    \(\mathbb{K}^{N}\) con \(\Span{x,y} = \sum_{i=1}^{N} x^{i}\overline{y^{i}} \) è prehilbertiano.
\end{example}
\begin{example}{}
    \(C^{0}{([0,1], \mathbb{C})}\) con \(\Span{f,g} = \int_{0}^{1} f{(x)}\overline{g}{(x)} \,d x \) 
\end{example}

\begin{definition}{Norma indotta dal prodotto scalare}
    Su uno spazio prehilbertiano \(X, \Span{\cdot ,\cdot } \) definisco 
    \[
      \|x\|:= \sqrt{\Span{x,x} } \quad \forall x \in X
    \]
    Allora \(\|\cdot \|\) è una norma su \(X\) 
\end{definition}
\begin{proof}[buona definizione]
    La radice è ben definita perché \(\Span{x,x}\) è un reale non negativo.
    Inoltre si può mostrare che \(\|\cdot \|\) è una norma con gli assiomi di
    prodotto scalare e la diseguaglianza di Schwarz per la diseguaglianza
    triangolare.
\end{proof}
\begin{proposition}{}
    Sia \(X\) uno spazio prehilbertiano, allora il prodotto scalare è una
    funzione continua \(X\times X \to \mathbb{K}\).
\end{proposition}
\begin{proof}{}
prese \(x_{n}\to x\) e \(y_{n}\to y\),
    \begin{align*}
        |\Span{x_{n},y_{n}} - \Span{x,y}| &= |\Span{x_{n},y_{n}} - \Span{x,y_{n}} +
      \Span{x,y_{n}} - \Span{x,y} | \\
            &= |\Span{x_{n}-x,y_{n}} + \Span{x,y_{n}-y} | \le |\Span{x_{n}-x,y_{n}} | + |\Span{x, y_{n}-y} | \le \\
            &\le \|x_{n}-x\|\|y_{n}\| + \|x\|\|y_{n}-y\|
            \overset{n\to \infty}{\longrightarrow }0
    \end{align*}
\end{proof}

\begin{definition}{ortogonalità}
    \(x,y \in X\) si dicono ortogonali se \(\Span{x,y} =0\) 
\end{definition}

\begin{proposition}[Identità di polarizzazione]
    Se \(\mathbb{K}=\mathbb{C}\),
    \[
      \Span{x,y} = \frac{1}{4}{\left( \|x+y\|^2 - \|x-y\|^2 + i\|x+iy\|^2 - i\|x-iy\|^2 \right)} 
    \]
    Se invece \(\mathbb{K}=\mathbb{R}\),
    \[
      \Span{x,y} = \frac{1}{4}{\left( \|x+y\|^2 - \|x-y\|^2\right)} 
    \]
\end{proposition}
\begin{proof}{}
    Non sono difficili, basta scrivere per esteso \(\|x+y\|^2\) e \(\|x-y\|^2\)
    (e \(\|x+iy\|^2\) e \(\|x-iy\|^2\) nel caso complesso) e poi fare i contazzi.
\end{proof}

\begin{proposition}{teorema di Pitagora}
    Se \(\Span{x,y} =0\) allora \(\|x+y\|^2 = \|x\|^2 + \|y\|^2\) 
\end{proposition}
\begin{proof}{}
    ovvio
\end{proof}
\begin{proposition}{Identità del parallelogramma}
    Per ogni \(x,y \in X\), allora
    \[
      \|x-y\|^2 + \|x+y\|^2 = 2{\left( \|x\|^2 + \|y\|^2 \right)} 
    \]
\end{proposition}

\begin{theorem}{Jordan - Von Neumann}
    Sia \(X\) uno spazio normato, allora la norma è indotta da un prodotto
    scalare se vale l'\emph{identità del parallelogramma}
\end{theorem}
\begin{proof}[Dimostrazione per \(\mathbb{K}=\mathbb{R}\) ]
    Definiamo il prodotto scalare con l'identità di polarizzazione, dunque
    \[
      \Span{x,y} := \frac{1}{4}{\left( \|x+y\|^2 - \|x-y\|^2 \right)} 
    \]
    infatti se effettivamente \(\Span{\cdot ,\cdot } \) è un prodotto scalare
    allora quest'uguaglianza varrebbe, dunque ha senso iniziare prendendola come
    definizione. Verifichiamo ora che è un prodotto scalare.
\begin{enumerate}[label = \roman*.]
    \item Evidente per definizione
    \item Evidente dalla definizione, perché viene letteralmente \(\Span{x,x} = \|x\|^2\) 
    \item Proseguiamo con la dimostrazione, dividendo in \(\Span{x+y,z} = \Span{x,z} +\Span{y,z} \) e \(\Span{\lambda x,y} = \lambda\Span{x,y} \) 
\end{enumerate}
    \begin{align*}
        \Span{x,z} + \Span{y,z} &\overset{(def)}{=} \frac{1}{4}{\left( \|x+z\|^2 - \|x-z\|^2 + \|y+z\|^2 - \|y-z\|^2 \right)} = \\
        &= \frac{1}{4}{\left( \|x+z\|^2 + \|y+z\|^2 \right)} - \frac{1}{4}{\left( \|x-z\|^2 + \|y-z\|^2 \right)} = \\
        &\overset{prll.}{=} \frac{1}{8}{\left(\cancel{ \|x-y\|^2} + \|x+y+2z\|^2 \right)} - \frac{1}{8}{\left( \cancel{\|x- y\|^2} + \|x + y - 2z\|^2 \right)} = \\
        &= \frac{2}{4} {\left( \left\| \frac{x+y}{2}+z \right\|^2 - \left\| \frac{x+y}{2} - z
\right\|^2\right)} = \\ &\overset{(def)}{=} 2\Span{\frac{x+y}{2}, z} 
    \end{align*}

    Da quest'ultima, scelto \(y=0\) e notando dalla definizione che \(\Span{0,z}=0\)  , abbiamo che
    \[
     \Span{x,z} = 2\Span{\frac{x}{2}, z} \implies \Span{x+y, z} = 2\Span{\frac{x+y}{2}, z} 
    \]
    che conclude la prima parte della dimostrazione della linearità.

    Procediamo definendo
    \[
      \Lambda = \{\lambda \in \mathbb{R} : \Span{\lambda x, y} = \lambda\Span{x,y} , \,\, \forall x,y \in X\} 
    \]
    allora chiaramente \(\{0, 1, -1\} \subseteq \Lambda\). Notiamo che se \(\alpha, \beta \in \Lambda\) allora \(\alpha + \beta \in \Lambda\):
    \[
      \Span{{(\alpha + \beta)}x, y} = \Span{\alpha x + \beta x, y} = \Span{\alpha x, y}  + \Span{\beta x, y} = \alpha \Span{x,y} + \beta \Span{x,y} = {(\alpha + \beta)}\Span{x,y} 
    \]
    Dunque necessariamente \(\mathbb{Z} \subseteq \Lambda \). Prendiamo ora \(\alpha, \beta \in \mathbb{Z}\) con \(\beta \neq 0\), allora
    \[
      \alpha\Span{x,y} = \Span{\alpha x, y} = \Span{\alpha \frac{\beta}{\beta}x,
      y } = \beta \Span{\frac{\alpha}{\beta}x, y} 
    \]
    da cui dividendo ambo i termini per \(\beta\) otteniamo che anche \(\mathbb{Q} \subseteq \Lambda \).
    Concludiamo che, poiché \(\mathbb{Q}\) è denso in \(\mathbb{R}\) e \(\Span{\cdot ,\cdot } \) è continuo (per come è definito, chiaramente non possiamo usare la prop, essendo che non abbiamo ancora dimostrato che \(\Span{\cdot ,\cdot } \) è  un prodotto scalare), allora \(\mathbb{R} \subseteq \Lambda \subseteq \).
\end{proof}
\begin{proof}[Dimostrazione per \(\mathbb{K}=\mathbb{C}\) ]
    Similmente a prima, definiamo 
    \[
      \Span{x,y} := \frac{1}{4}{\left( \|x+y\|^2 - \|x-y\|^2  \right)}+ i \frac{1}{4}{\left(  \|x+iy\|^2 - \|x-iy\|^2 \right)} 
    \]
    Dunque \(Re\Span{x,y} = \frac{1}{4}{\left( \|x+y\|^2 - \|x-y\|^2 \right)} =: (x,y) \). Allora 
    \[
      \Span{x,y} = (x,y) + i (x, iy)
    \]
    allora per la parte reale del teorema \({(x,y)}\) verifica \({(x+y, z)} = {(x,z)}+{(y,z)}\) e \({(\lambda x, y)} = \lambda {(x,y)}\) per ogni \(\lambda \in \mathbb{R}\). Dunque
    \[
      \Span{x+y, z}  = {(x,z)} + {(y,z)} + i{(x,iz)} i{(y,iz)} = \Span{x,z} + \Span{y,z} 
    \]
    Rimane da verificare l'omogeneità per \(\lambda \in \mathbb{C}\) e che \(\Span{x,y} = \overline{\Span{y,x} }\). Iniziamo dalla seconda:
    \[
      \overline{\Span{y,x} } = \overline{{(y,x)} + i {(y, ix)}} = {(y,x)} - i {(y, ix)}
    \]
    inolre
    \begin{align*}
        {(y,ix)} &= \frac{1}{4}{\left( \|y + ix\|^2 - \|y -ix\|^2 \right)} = \frac{1}{4}{\left( \|i{(-iy + x)}\|^2 + \|i{(-iy - x)}\|^2 \right)} = \\
                 &= \frac{1}{4}{\left( \|x - iy\|^2 + \|x + iy\|^2 \right)} =
                 -{(x, iy)}
    \end{align*}
    e quindi la precedente è
    \[
      \overline{\Span{y,x} } = {(x,y)} + i{(x, iy)} = \Span{x,y} 
    \]

    Sia ora \(\alpha + i \beta = \lambda \in \mathbb{C}\), con \(\alpha , \beta \in \mathbb{R}\). Allora
    \[
    \Span{{(\alpha + i \beta)}x, y} = \Span{\alpha x + i \beta x, y} = \Span{ \alpha x , y } + i \Span{\beta x , y} = \alpha \Span{x,y} + \beta \Span{ix, y} 
    \]
    ma abbiamo che, riprendendo la definizione
    \begin{align*}
        \Span{ix, y} &= \frac{1}{4}{\left( \|ix + y\|^2 - \|ix-y\|^2 \right)} + i
      \frac{1}{4}{\left( \|ix + iy\|^2 - \|ix -iy\|^2 \right)} \\
                     &= -\frac{1}{4}{\left( \|x+iy\|^2 - \|x - iy\|^2 \right)} +
        i {(x,y)} = i{(x,y)} - {(x, iy)} = \\ &= i\Span{x,y} 
    \end{align*}

    e quindi concludiamo
    \[
      \Span{{(\alpha + i \beta)}x, y} = \alpha\Span{x,y} + \beta\Span{ix,y} = \alpha\Span{x,y} +i \beta\Span{x,y} = \lambda\Span{x,y} 
    \]
    

\end{proof}

\begin{remark}{}
    presa su \(C^{0}{([0,1])}\) la norma \(\|f\|_2^2 = \int_{0}^{1} |f|^2 \,dt \), allora \\\(\Span{f,g} = \int_{0}^{1} f{(t)}g{(t)} \,dt \) questo è uno spazio prehilbertiano.

    Però con la norma \(\|f\|_{\infty}\) non è uno spazio prehilbertiano.
    Infatti non vale l'identità del parallelogramma: prese \(f{(t)}=1-t\) e \(g{(t)}= t\) abbiamo
\[
  \|f-g\|^2_{\infty} + \|f+g\|^2_\infty = 1+1\neq  2{(1 + 1)} = 2{(\|f\|^2_\infty + \|g\|^2_\infty)}
\]
\end{remark}
\begin{corollary}{}
    Sia \(X\) uno spazio normato e sia \(M \subseteq X  \) un sottospazio di
    dimensione finita. Allora \(M\) è chiuso.
\end{corollary}
\begin{proof}{}
    \({(M, \|\cdot \|)}\) è esso stesso uno spazio normato di dimensione finita.
    \(M\) è dunque completo quindi chiuso.
\end{proof}

\begin{example}{}
    La precedente non vale se \(\dim X = +\infty\). Presi infatti \(M = C^{0}{(\Omega)}\) e \(X = L^2{(\Omega)}\), abbiamo che \(\overline{M}^{L^2} = L^2\) 
\end{example}

\subsection{Operatori lineari e continui}

Siano \(X\) e \(Y\) spazi normati. Sia \(
T: X\to Y\). Allora \(T\) è lineare se \[T{(\alpha x + \beta y)} = \alpha T{(x)} + \beta T{(y)}\] per ogni \(\alpha, \beta \in \mathbb{K}\) e \(x,y \in X\). Per ricordare la linearità, invece di scrivere \(T{(x)}\) scriveremo \(Tx\).

\begin{note}{zione}
    Nelle bolle, indicando a pedice lo spazio invece che il raggio, si sottintende il raggio
    1 e si esplicita la norma da utilizzare:
\[
  B_X{(0)} := \{x \in  X : \|x\|_X < 1\} 
\]
\end{note}

\begin{theorem}{}
    Siano \(X, Y\) spazi normati. Sia \(T : X \to Y\). Allora le seguenti
    proposizioni sono tutte equivalenti:
\begin{enumerate}[label = (\roman*)]
    \item \(T\) è continuo
    \item \(T\) è continuo in 0
    \item Ogni limitato di \(X\) ha immagine limitata in \(Y\) 
\item \(\displaystyle \exists \alpha>0 : \overline{T{(B_X)}} \subseteq \alpha B_Y{(0)}\) 
    \item \(\displaystyle \sup_{X \sminus \{0\} } \frac{\|Tx\|_y}{\|x\|_X} < +\infty\) 
    \item \(\displaystyle \sup_{x \in B_X{(0)}} \|Tx\|_Y < +\infty\) 
    \item \(\displaystyle\sup_{\|x\|_X=1} \|Tx\|_Y < +\infty\) 
\end{enumerate}
\end{theorem}
\begin{remark}{}
    Se \(X\) e \(Y\) hanno dimensione finita, \(T\) è sempre continuo.
\end{remark}
\begin{example}{}
    Preso \begin{align*}
        T: C^{0}{([0,1])}_{\|\cdot \|_1}  &\longrightarrow \mathbb{R} \\
        f &\longmapsto T(f) = f{(0)}
    \end{align*}
    è chiaramente lineare. Tuttavia la controimmagine di \(\{0\} \) tramite \(T\) contiene
    ad esempio la successione \(f_{n}{(x)} = \max(nx,1)\) che ha come limite in \(C^{0}_{\|\cdot\|_1} \) la funzione costante 1, per cui \(T^{-1}\{0\} \) non è chiuso.
\end{example}
\begin{definition}{Operatore limitato}
    Un operatore che soddisfa la condizione \({(iii)}\) viene detto
    \textbf{limitato}
\end{definition}
\begin{proof}{} \( \)
\begin{itemize}[label = --]
    \item[\((i) \implies (ii)\)] ovvio
    \item[\((ii)\implies (i)\) ] ovvio, poiché \(T{(x-x_{0})} = T{(x)} - T{(x_{0})}\) 
    \item[\((ii) \implies (iv)\)] Abbiamo che per ogni intorno \(U_Y\) di \(0_Y\) esiste un intorno \(U_X\) di \(0_X\) tale che \(T{(U_X)} \subseteq U_Y \). Allora scelto \(U_Y = \overline{B_Y{(0)}}\) abbiamo
        \[
          \exists \delta > 0 : T{(\delta\overline{B_X{(0)}})} = \delta T{(\overline{B_X{(0)}})}\subseteq B_Y{(0)} 
        \]
        per cui basta prendere \(\alpha = \frac{1}{\delta}\) 
    \item[\((iv) \implies (ii)\)] Preso \(\varepsilon > 0\) bisogna trovare \(\delta>0\) tale che 
        \[
          T{(\delta \overline{B_x{(0)}})} \subseteq \varepsilon B_Y{(0)} 
        \]
        e similmente a prima per linearità basta prendere \(\delta = \varepsilon / \alpha\) 
    \item[\((iv) \implies (iii)\)] Sia \(C \subseteq R \overline{B_X{(0)}} \) un
        limitato. Allora
        \[
          T{(C)} \subseteq T{(R \overline{B_X{(0)}})} = RT{(\overline{B_x{(0)}})} \subseteq R \alpha \overline{B_Y{(0)}}  
        \]
    \item[\((iii) \implies (iv)\)] \(\overline{B_X}{(0)}\) è limitato in \(X\),
        dunque \(T{(\overline{B_x{(0)}})}\) è limitato in \(Y\), e dunque è
        contenuto in una palla \(\alpha B_Y{(0)}\) per un \(\alpha > 0\) 
    \item[\((iv) \iff (vi)\)] \(\|x\|_X \le 1\) se e solo se \(x \in \overline{B_X{(0)}}\), il resto vien da sè
    \item[\((v) \iff (vi) \iff (vii)\)] tutte ovvie, come anche è ovvio che il
        valore finito nel caso sia lo stesso, e viene denotato \(\|T\| \) e in pratica tutte e tre dicono che \[\exists \|T\| > 0 : \|Tx\|_Y \le \|T\| \|x\|_X \] per ogni \(x \in X\) 
\end{itemize}
\end{proof}





